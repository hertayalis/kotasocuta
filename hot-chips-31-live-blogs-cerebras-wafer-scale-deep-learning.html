<!doctype html><html lang=en><head><meta charset=utf-8><meta name=HandheldFriendly content="True"><meta name=MobileOptimized content="320"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=referrer content="no-referrer"><meta name=description content="08:49PM EDT - Some of the big news of today is Cerebras announcing its wafer-scale 1.2 trillion transistor solution for deep learning. The talk today goes into detail about the technology. 08:51PM EDT - Wafer scale chip, over 46,225 mm2, 1.2 trillion transistors, 400k AI cores, fed by 18GB of on-chip SRAM"><meta name=robots content="index,follow,noarchive"><link href="https://fonts.googleapis.com/css?family=Open+Sans:400|Old+Standard+TT:400&display=swap" rel=stylesheet media=print type=text/css onload='this.media="all"'><title>Cerebras' 1.2 Trillion Transistor Deep Learning Processor</title><link rel=canonical href=./hot-chips-31-live-blogs-cerebras-wafer-scale-deep-learning.html><style>*{border:0;font:inherit;font-size:100%;vertical-align:baseline;margin:0;padding:0;color:#000;text-decoration-skip:ink}body{font-family:open sans,myriad pro,Myriad,sans-serif;font-size:17px;line-height:160%;color:#1d1313;max-width:700px;margin:auto}p{margin:20px 0}a img{border:none}img{margin:10px auto;max-width:100%;display:block}.left-justify{float:left}.right-justify{float:right}pre,code{font:12px Consolas,liberation mono,Menlo,Courier,monospace;background-color:#f7f7f7}code{font-size:12px;padding:4px}pre{margin-top:0;margin-bottom:16px;word-wrap:normal;padding:16px;overflow:auto;font-size:85%;line-height:1.45}pre>code{padding:0;margin:0;font-size:100%;word-break:normal;white-space:pre;background:0 0;border:0}pre code{display:inline;padding:0;margin:0;overflow:visible;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}pre code::before,pre code::after{content:normal}em,q,em,dfn{font-style:italic}.sans,html .gist .gist-file .gist-meta{font-family:open sans,myriad pro,Myriad,sans-serif}.mono,pre,code,tt,p code,li code{font-family:Menlo,Monaco,andale mono,lucida console,courier new,monospace}.heading,.serif,h1,h2,h3{font-family:old standard tt,serif}strong{font-weight:600}q:before{content:"\201C"}q:after{content:"\201D"}del,s{text-decoration:line-through}blockquote{font-family:old standard tt,serif;text-align:center;padding:50px}blockquote p{display:inline-block;font-style:italic}blockquote:before,blockquote:after{font-family:old standard tt,serif;content:'\201C';font-size:35px;color:#403c3b}blockquote:after{content:'\201D'}hr{width:40%;height:1px;background:#403c3b;margin:25px auto}h1{font-size:35px}h2{font-size:28px}h3{font-size:22px;margin-top:18px}h1 a,h2 a,h3 a{text-decoration:none}h1,h2{margin-top:28px}#sub-header,.date{color:#403c3b;font-size:13px}#sub-header{margin:0 4px}#nav h1 a{font-size:35px;color:#1d1313;line-height:120%}.posts_listing a,#nav a{text-decoration:none}li{margin-left:20px}ul li{margin-left:5px}ul li{list-style-type:none}ul li:before{content:"\00BB \0020"}#nav ul li:before,.posts_listing li:before{content:'';margin-right:0}#content{text-align:left;width:100%;font-size:15px;padding:60px 0 80px}#content h1,#content h2{margin-bottom:5px}#content h2{font-size:25px}#content .entry-content{margin-top:15px}#content .date{margin-left:3px}#content h1{font-size:30px}.highlight{margin:10px 0}.posts_listing{margin:0 0 50px}.posts_listing li{margin:0 0 25px 15px}.posts_listing li a:hover,#nav a:hover{text-decoration:underline}#nav{text-align:center;position:static;margin-top:60px}#nav ul{display:table;margin:8px auto 0}#nav li{list-style-type:none;display:table-cell;font-size:15px;padding:0 20px}#links{display:flex;justify-content:space-between;margin:50px 0 0}#links :nth-child(1){margin-right:.5em}#links :nth-child(2){margin-left:.5em}#not-found{text-align:center}#not-found a{font-family:old standard tt,serif;font-size:200px;text-decoration:none;display:inline-block;padding-top:225px}@media(max-width:750px){body{padding-left:20px;padding-right:20px}#nav h1 a{font-size:28px}#nav li{font-size:13px;padding:0 15px}#content{margin-top:0;padding-top:50px;font-size:14px}#content h1{font-size:25px}#content h2{font-size:22px}.posts_listing li div{font-size:12px}}@media(max-width:400px){body{padding-left:20px;padding-right:20px}#nav h1 a{font-size:22px}#nav li{font-size:12px;padding:0 10px}#content{margin-top:0;padding-top:20px;font-size:12px}#content h1{font-size:20px}#content h2{font-size:18px}.posts_listing li div{font-size:12px}}@media(prefers-color-scheme:dark){*,#nav h1 a{color:#fdfdfd}body{background:#121212}pre,code{background-color:#262626}#sub-header,.date{color:#bababa}hr{background:#ebebeb}}</style></head><body><section id=nav><h1><a href=./index.html>PeakDash</a></h1><ul><li><a href=./index.xml>Rss</a></li><li><a href=./sitemap.xml>Sitemap</a></li></ul></section><section id=content><h1>Cerebras' 1.2 Trillion Transistor Deep Learning Processor</h1><div id=sub-header>May 2024 Â· 5 minute read</div><div class=entry-content><p><a href=# id=post0819204952><span class=lb_time>08:49PM EDT</span></a> - Some of the big news of today is Cerebras announcing its wafer-scale 1.2 trillion transistor solution for deep learning. The talk today goes into detail about the technology.</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_175010_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819205102><span class=lb_time>08:51PM EDT</span></a> - Wafer scale chip, over 46,225 mm2, 1.2 trillion transistors, 400k AI cores, fed by 18GB of on-chip SRAM</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_175016_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819205119><span class=lb_time>08:51PM EDT</span></a> - TSMC 16nm</p><p><a href=# id=post0819205137><span class=lb_time>08:51PM EDT</span></a> - 215mm x 215mm - 8.5 inches per side</p><p><a href=# id=post0819205152><span class=lb_time>08:51PM EDT</span></a> - 56 times larger than the largest GPU today</p><p><a href=# id=post0819205227><span class=lb_time>08:52PM EDT</span></a> - Built for Deep Learning</p><p><a href=# id=post0819205231><span class=lb_time>08:52PM EDT</span></a> - DL training is hard (ed: this is an understatement)</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_175210_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819205255><span class=lb_time>08:52PM EDT</span></a> - Peta-to-exa scale compute range</p><p><a href=# id=post0819205302><span class=lb_time>08:53PM EDT</span></a> - The shape of the problem is difficult to scale</p><p><a href=# id=post0819205311><span class=lb_time>08:53PM EDT</span></a> - Fine grain has a lot of parallelism</p><p><a href=# id=post0819205317><span class=lb_time>08:53PM EDT</span></a> - Coarse grain is inherently serial</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_175236_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819205327><span class=lb_time>08:53PM EDT</span></a> - Training is the process of applying small changes, serially</p><p><a href=# id=post0819205337><span class=lb_time>08:53PM EDT</span></a> - Size and shape of the problem makes training NN really hard</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_175342_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819205358><span class=lb_time>08:53PM EDT</span></a> - Today we have dense vector compute</p><p><a href=# id=post0819205416><span class=lb_time>08:54PM EDT</span></a> - For Coarse Grained, require high speed interconnect to run mutliple instances. Still limited</p><p><a href=# id=post0819205422><span class=lb_time>08:54PM EDT</span></a> - Scaling is limited and costly</p><p><a href=# id=post0819205455><span class=lb_time>08:54PM EDT</span></a> - Specialized accelerators are the answer</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_175439_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819205504><span class=lb_time>08:55PM EDT</span></a> - NN: what is the right architecture</p><p><a href=# id=post0819205528><span class=lb_time>08:55PM EDT</span></a> - Need a core to be optimized for NN primitives</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_175510_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819205532><span class=lb_time>08:55PM EDT</span></a> - Need a programmable NN core</p><p><a href=# id=post0819205538><span class=lb_time>08:55PM EDT</span></a> - Needs to do sparse compute fast</p><p><a href=# id=post0819205542><span class=lb_time>08:55PM EDT</span></a> - Needs fast local memory</p><p><a href=# id=post0819205552><span class=lb_time>08:55PM EDT</span></a> - All of the cores should be connected with a fast interconnect</p><p><a href=# id=post0819205620><span class=lb_time>08:56PM EDT</span></a> - Cerebras uses flexible cores. Flexible general ops for control processing</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_175559_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819205629><span class=lb_time>08:56PM EDT</span></a> - Core should handle tensor operations very efficiency</p><p><a href=# id=post0819205637><span class=lb_time>08:56PM EDT</span></a> - Forms the bulk fo the compute in any neural network</p><p><a href=# id=post0819205645><span class=lb_time>08:56PM EDT</span></a> - Tensors as first class operands</p><p><a href=# id=post0819205720><span class=lb_time>08:57PM EDT</span></a> - fmac native op</p><p><a href=# id=post0819205758><span class=lb_time>08:57PM EDT</span></a> - NN naturally creates sparse networks</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_175738_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819205810><span class=lb_time>08:58PM EDT</span></a> - The core has native sparse processing in the hardware with dataflow scheduling</p><p><a href=# id=post0819205815><span class=lb_time>08:58PM EDT</span></a> - All the compute is triggered by the data</p><p><a href=# id=post0819205823><span class=lb_time>08:58PM EDT</span></a> - Filters all the sparse zeros, and filters the work</p><p><a href=# id=post0819205838><span class=lb_time>08:58PM EDT</span></a> - saves the power and energy, and get performance and acceleration by moving onto the next useful work</p><p><a href=# id=post0819205848><span class=lb_time>08:58PM EDT</span></a> - Enabled because arch has fine-grained execution datapaths</p><p><a href=# id=post0819205855><span class=lb_time>08:58PM EDT</span></a> - Many small cores with independent instructions</p><p><a href=# id=post0819205901><span class=lb_time>08:59PM EDT</span></a> - Allows for very non-uniform work</p><p><a href=# id=post0819205905><span class=lb_time>08:59PM EDT</span></a> - Next is memory</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_175910_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819205934><span class=lb_time>08:59PM EDT</span></a> - Traditional memory architectures are not optimized for DL</p><p><a href=# id=post0819205946><span class=lb_time>08:59PM EDT</span></a> - Traditional memory requires high data reuse for performane</p><p><a href=# id=post0819210004><span class=lb_time>09:00PM EDT</span></a> - Normal matrix multiply has low end data reuse</p><p><a href=# id=post0819210028><span class=lb_time>09:00PM EDT</span></a> - Translating Mat*Vec into Mat*Mat, but changes the training dynamics</p><p><a href=# id=post0819210044><span class=lb_time>09:00PM EDT</span></a> - Cerebras has high-perf, fully distributed on-chip SRAM next to the cores</p><p><a href=# id=post0819210106><span class=lb_time>09:01PM EDT</span></a> - Getting orders of magnitude higher bandwidth</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_180051_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819210120><span class=lb_time>09:01PM EDT</span></a> - ML can be done the way it wants to be done</p><p><a href=# id=post0819210141><span class=lb_time>09:01PM EDT</span></a> - High bandwidth, low latency interconnect</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_180126_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819210147><span class=lb_time>09:01PM EDT</span></a> - fast and fully configurable fabric</p><p><a href=# id=post0819210158><span class=lb_time>09:01PM EDT</span></a> - all hw based communication avoicd sw overhead</p><p><a href=# id=post0819210205><span class=lb_time>09:02PM EDT</span></a> - 2D mesh topology</p><p><a href=# id=post0819210218><span class=lb_time>09:02PM EDT</span></a> - higher utlization and efficiency than global topologies</p><p><a href=# id=post0819210248><span class=lb_time>09:02PM EDT</span></a> - Need more than a single die</p><p><a href=# id=post0819210254><span class=lb_time>09:02PM EDT</span></a> - Solition is a wafer scale</p><p><a href=# id=post0819210315><span class=lb_time>09:03PM EDT</span></a> - Build Big chips</p><p><a href=# id=post0819210321><span class=lb_time>09:03PM EDT</span></a> - Cluster scale perf on a single chip</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_180258_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819210330><span class=lb_time>09:03PM EDT</span></a> - GB of fast memory (SRAM) 1 clock cycle from the core</p><p><a href=# id=post0819210336><span class=lb_time>09:03PM EDT</span></a> - That's impossible with off-chip memory</p><p><a href=# id=post0819210343><span class=lb_time>09:03PM EDT</span></a> - Full on-chip interconnect fabric</p><p><a href=# id=post0819210356><span class=lb_time>09:03PM EDT</span></a> - Model parallel, linear performance scaling</p><p><a href=# id=post0819210406><span class=lb_time>09:04PM EDT</span></a> - Map the entire neural network onto the chip at once</p><p><a href=# id=post0819210418><span class=lb_time>09:04PM EDT</span></a> - One instance of NN, don't have to increase batch size to get cluster scale perf</p><p><a href=# id=post0819210425><span class=lb_time>09:04PM EDT</span></a> - Vastly lower power and less space</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_180435_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819210457><span class=lb_time>09:04PM EDT</span></a> - Can use TensorFlow and PyTorch</p><p><a href=# id=post0819210512><span class=lb_time>09:05PM EDT</span></a> - Performs placing and routing to map neural network layers to fabric</p><p><a href=# id=post0819210522><span class=lb_time>09:05PM EDT</span></a> - Entire wafer operates on the single neural network</p><p><a href=# id=post0819210529><span class=lb_time>09:05PM EDT</span></a> - Challenges of wafer scale</p><p><a href=# id=post0819210557><span class=lb_time>09:05PM EDT</span></a> - Need cross-die connectivity, yield, thermal expansion</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_180534_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_180630_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819210655><span class=lb_time>09:06PM EDT</span></a> - Scribe line separates the die. On top of the scribe line, create wires</p><p><a href=# id=post0819210702><span class=lb_time>09:07PM EDT</span></a> - Extends 2D mesh fabric across all die</p><p><a href=# id=post0819210711><span class=lb_time>09:07PM EDT</span></a> - Same connectivity between cores and between die</p><p><a href=# id=post0819210732><span class=lb_time>09:07PM EDT</span></a> - More efficient than off-chip</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_180719_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819210747><span class=lb_time>09:07PM EDT</span></a> - Full BW at the die level</p><p><a href=# id=post0819210810><span class=lb_time>09:08PM EDT</span></a> - Redundancy helps yield</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_180753_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819210831><span class=lb_time>09:08PM EDT</span></a> - Redundant cores and redundant fabric links</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_180816_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819210848><span class=lb_time>09:08PM EDT</span></a> - Reconnect the fabric with links</p><p><a href=# id=post0819210855><span class=lb_time>09:08PM EDT</span></a> - Drive yields high</p><p><a href=# id=post0819210902><span class=lb_time>09:09PM EDT</span></a> - Transparent to software</p><p><a href=# id=post0819210924><span class=lb_time>09:09PM EDT</span></a> - Thermal expansion</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_180909_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819210935><span class=lb_time>09:09PM EDT</span></a> - Normal tech, too much mechanical stress via thermal expansion</p><p><a href=# id=post0819210939><span class=lb_time>09:09PM EDT</span></a> - Custom connector developed</p><p><a href=# id=post0819210948><span class=lb_time>09:09PM EDT</span></a> - Connector absorbs the variation in thermal expansion</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_181005_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819211026><span class=lb_time>09:10PM EDT</span></a> - All components need to be held with precise alignment - custom packaging tools</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_181031_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819211049><span class=lb_time>09:10PM EDT</span></a> - Power and Cooling</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_181056_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819211117><span class=lb_time>09:11PM EDT</span></a> - Power planes don't work - isn't enough copper in the PCB to do it that way</p><p><a href=# id=post0819211128><span class=lb_time>09:11PM EDT</span></a> - Heat density too high for direct air cooling</p><p><a href=# id=post0819211206><span class=lb_time>09:12PM EDT</span></a> - Bring current perpendicular to the wafer. Water cooled perpendicular too</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_181142_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_181224_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819211406><span class=lb_time>09:14PM EDT</span></a> - Q&A Time</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_181341_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819211445><span class=lb_time>09:14PM EDT</span></a> - Q and A</p><p><a href=# id=post0819211458><span class=lb_time>09:14PM EDT</span></a> - Already in use? Yes</p><p><a href=# id=post0819211515><span class=lb_time>09:15PM EDT</span></a> - Can you make a round chip? Square is more convenient</p><p><a href=# id=post0819211558><span class=lb_time>09:15PM EDT</span></a> - Yield? Mature processes are quite good and uniform</p><p><a href=# id=post0819211658><span class=lb_time>09:16PM EDT</span></a> - Does it cost less than a house? Everything is amortised across the wafer</p><p><a href=# id=post0819211722><span class=lb_time>09:17PM EDT</span></a> - Regular processor for housekeeping? They can all do it</p><p><a href=# id=post0819211741><span class=lb_time>09:17PM EDT</span></a> - Is it fully synchronous? No</p><p><a href=# id=post0819212001><span class=lb_time>09:20PM EDT</span></a> - Clock rate? Not disclosed</p><p><a href=# id=post0819212042><span class=lb_time>09:20PM EDT</span></a> - That's a wrap. Next is Habana</p><p class=postsid style=color:rgba(255,0,0,0)>ncG1vNJzZmivp6x7orrAp5utnZOde6S7zGiqoaenZH51g5RxZqGnpGKwqbXPrGRsaV2htrexjJujqJ%2BjYrCmvsSbqZqrXayup7HRZqqcmZyaeqWxxKlkpZ2Rp7uqusY%3D</p></div><div id=links><a href=./eva-longoria-bio-parents-siblings-husband-net-worth-height.html>&#171;&nbsp;Eva Longoria Bio, Parents, Siblings, Husband, Net Worth, Height</a>
<a href=./all-tracker-cbs-cast-members-confirmed.html>All Tracker CBS cast members, confirmed&nbsp;&#187;</a></div></section><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://iklan.listspress.com/floating.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://iklan.listspress.com/tracking_server_6.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script>var _paq=window._paq=window._paq||[];_paq.push(["trackPageView"]),_paq.push(["enableLinkTracking"]),function(){e="//analytics.cdnweb.info/",_paq.push(["setTrackerUrl",e+"matomo.php"]),_paq.push(["setSiteId","1"]);var e,n=document,t=n.createElement("script"),s=n.getElementsByTagName("script")[0];t.async=!0,t.src=e+"matomo.js",s.parentNode.insertBefore(t,s)}()</script></body></html>